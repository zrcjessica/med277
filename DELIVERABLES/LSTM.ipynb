{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer \n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in datasets\n",
    "train_original = pd.read_csv('train.csv')\n",
    "train = pd.read_csv('train_medical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove english stopwords\n",
    "def remove_stopwords(words):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word.lower() for word in words if word not in stop_words]\n",
    "\n",
    "# remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# preprocess and balance dataset\n",
    "def Preprocess(train):\n",
    "    sample_size = len(train[train.target == 1])\n",
    "    train_balanced = train[train.target == 1].sample(sample_size).append(train[train.target == 0].sample(sample_size)).reset_index()\n",
    "    train_balanced = train_balanced.drop(columns=['index'])\n",
    "    # Removing punctuation\n",
    "    train_balanced['question_text_token'] = train_balanced['question_text'].apply(lambda x: remove_punctuation(x))\n",
    "    # Tokenizing the text\n",
    "    train_balanced['question_text_token'] = train_balanced['question_text_token'].apply(lambda x: word_tokenize(x))\n",
    "    # Removing stopwords\n",
    "    train_balanced['question_text_token'] = train_balanced['question_text_token'].apply(lambda x: remove_stopwords(x))\n",
    "    return train_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_original = Preprocess(train_original)\n",
    "train_balanced = Preprocess(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google news word2vec model\n",
    "google_word2vec = 'GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "# load word2vec model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(google_word2vec, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorization(question_token):\n",
    "    words = [i for i in question_token if i in word2vec_model]\n",
    "    vector_representations = [word2vec_model[i] for i in words]\n",
    "    return vector_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cleaner(tokens):\n",
    "    tokens = [word for word in tokens if word in word2vec_model]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_original['question_text_vector'] = train_balanced_original['question_text_token'].apply(Vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_original['question_text_clean'] = train_balanced_original['question_text_token'].apply(Cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced['question_text_vector'] = train_balanced['question_text_token'].apply(Vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced['question_text_clean'] = train_balanced['question_text_token'].apply(Cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_original = train_balanced_original.sample(frac=1).reset_index()\n",
    "train_balanced_original = train_balanced_original.drop(columns=['index'])\n",
    "fraction = 0.5\n",
    "train_balanced_original_train = train_balanced_original.iloc[:int(fraction*len(train_balanced_original))]\n",
    "train_balanced_original_test = train_balanced_original.iloc[int(fraction*len(train_balanced_original)):]\n",
    "Y_train = np.array(train_balanced_original_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(set([i for j in list(train_balanced_original_train['question_text_clean'])for i in j.split(' ')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create sequence\n",
    "vocabulary_size = vocab_size\n",
    "tokenizer = Tokenizer(num_words= vocab_size)\n",
    "tokenizer.fit_on_texts(train_balanced_original_train['question_text_clean'])\n",
    "sequences = tokenizer.texts_to_sequences(train_balanced_original_train['question_text_clean'])\n",
    "data = pad_sequences(sequences, maxlen=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = word2vec_model[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 300, input_length=32, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(300))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48486 samples, validate on 32324 samples\n",
      "Epoch 1/3\n",
      "48486/48486 [==============================] - 33s 679us/step - loss: 0.3342 - acc: 0.8625 - val_loss: 0.3248 - val_acc: 0.8704\n",
      "Epoch 2/3\n",
      "48486/48486 [==============================] - 31s 648us/step - loss: 0.2873 - acc: 0.8858 - val_loss: 0.3087 - val_acc: 0.8749\n",
      "Epoch 3/3\n",
      "48486/48486 [==============================] - 32s 652us/step - loss: 0.2569 - acc: 0.8994 - val_loss: 0.3111 - val_acc: 0.8769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa3d46762b0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, Y_train, validation_split=0.4, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LSTMBalanced_original.pkl']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(train_balanced_original_train, 'lstm_original_train.pkl')\n",
    "joblib.dump(train_balanced_original_test, 'lstm_original_test.pkl')\n",
    "joblib.dump(model, 'LSTMBalanced_original.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced = train_balanced.sample(frac=1).reset_index()\n",
    "train_balanced = train_balanced.drop(columns=['index'])\n",
    "fraction = 0.5\n",
    "train_balanced_train = train_balanced.iloc[:int(fraction*len(train_balanced))]\n",
    "train_balanced_test = train_balanced.iloc[int(fraction*len(train_balanced)):]\n",
    "Y_train = np.array(train_balanced_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(set([i for j in list(train_balanced_train['question_text_clean'])for i in j.split(' ')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create sequence\n",
    "vocabulary_size = vocab_size\n",
    "tokenizer = Tokenizer(num_words= vocab_size)\n",
    "tokenizer.fit_on_texts(train_balanced_train['question_text_clean'])\n",
    "sequences = tokenizer.texts_to_sequences(train_balanced_train['question_text_clean'])\n",
    "data = pad_sequences(sequences, maxlen=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = word2vec_model[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 300, input_length=32, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(300))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2035 samples, validate on 1357 samples\n",
      "Epoch 1/3\n",
      "2035/2035 [==============================] - 3s 2ms/step - loss: 0.4658 - acc: 0.7843 - val_loss: 0.3214 - val_acc: 0.8710\n",
      "Epoch 2/3\n",
      "2035/2035 [==============================] - 1s 654us/step - loss: 0.2884 - acc: 0.8919 - val_loss: 0.2966 - val_acc: 0.8784\n",
      "Epoch 3/3\n",
      "2035/2035 [==============================] - 1s 677us/step - loss: 0.2169 - acc: 0.9219 - val_loss: 0.3012 - val_acc: 0.8755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa1f5b1df28>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, Y_train, validation_split=0.4, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LSTMBalanced.pkl']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(train_balanced_train, 'lstm_train.pkl')\n",
    "joblib.dump(train_balanced_test, 'lstm_test.pkl')\n",
    "joblib.dump(model, 'LSTMBalanced.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
